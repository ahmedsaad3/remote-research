{
  "2304.05133v2": {
    "title": "Lecture Notes: Neural Network Architectures",
    "authors": [
      "Evelyn Herberg"
    ],
    "summary": "These lecture notes provide an overview of Neural Network architectures from\na mathematical point of view. Especially, Machine Learning with Neural Networks\nis seen as an optimization problem. Covered are an introduction to Neural\nNetworks and the following architectures: Feedforward Neural Network,\nConvolutional Neural Network, ResNet, and Recurrent Neural Network.",
    "pdf_url": "http://arxiv.org/pdf/2304.05133v2",
    "published": "2023-04-11"
  },
  "cs/0504056v1": {
    "title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity",
    "authors": [
      "V. Schetinin"
    ],
    "summary": "The principles of self-organizing the neural networks of optimal complexity\nis considered under the unrepresentative learning set. The method of\nself-organizing the multi-layered neural networks is offered and used to train\nthe logical neural networks which were applied to the medical diagnostics.",
    "pdf_url": "http://arxiv.org/pdf/cs/0504056v1",
    "published": "2005-04-13"
  },
  "1911.05640v2": {
    "title": "Neural Network Processing Neural Networks: An efficient way to learn higher order functions",
    "authors": [
      "Firat Tuna"
    ],
    "summary": "Functions are rich in meaning and can be interpreted in a variety of ways.\nNeural networks were proven to be capable of approximating a large class of\nfunctions[1]. In this paper, we propose a new class of neural networks called\n\"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural\nnetworks and numerical values, instead of just numerical values. Thus enabling\nneural networks to represent and process rich structures.",
    "pdf_url": "http://arxiv.org/pdf/1911.05640v2",
    "published": "2019-11-06"
  },
  "2304.13812v1": {
    "title": "Guaranteed Quantization Error Computation for Neural Network Model Compression",
    "authors": [
      "Wesley Cooke",
      "Zihao Mo",
      "Weiming Xiang"
    ],
    "summary": "Neural network model compression techniques can address the computation issue\nof deep neural networks on embedded devices in industrial systems. The\nguaranteed output error computation problem for neural network compression with\nquantization is addressed in this paper. A merged neural network is built from\na feedforward neural network and its quantized version to produce the exact\noutput difference between two neural networks. Then, optimization-based methods\nand reachability analysis methods are applied to the merged neural network to\ncompute the guaranteed quantization error. Finally, a numerical example is\nproposed to validate the applicability and effectiveness of the proposed\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2304.13812v1",
    "published": "2023-04-26"
  },
  "2007.06559v2": {
    "title": "Graph Structure of Neural Networks",
    "authors": [
      "Jiaxuan You",
      "Jure Leskovec",
      "Kaiming He",
      "Saining Xie"
    ],
    "summary": "Neural networks are often represented as graphs of connections between\nneurons. However, despite their wide use, there is currently little\nunderstanding of the relationship between the graph structure of the neural\nnetwork and its predictive performance. Here we systematically investigate how\ndoes the graph structure of neural networks affect their predictive\nperformance. To this end, we develop a novel graph-based representation of\nneural networks called relational graph, where layers of neural network\ncomputation correspond to rounds of message exchange along the graph structure.\nUsing this representation we show that: (1) a \"sweet spot\" of relational graphs\nleads to neural networks with significantly improved predictive performance;\n(2) neural network's performance is approximately a smooth function of the\nclustering coefficient and average path length of its relational graph; (3) our\nfindings are consistent across many different tasks and datasets; (4) the sweet\nspot can be identified efficiently; (5) top-performing neural networks have\ngraph structure surprisingly similar to those of real biological neural\nnetworks. Our work opens new directions for the design of neural architectures\nand the understanding on neural networks in general.",
    "pdf_url": "http://arxiv.org/pdf/2007.06559v2",
    "published": "2020-07-13"
  }
}